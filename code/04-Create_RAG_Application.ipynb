{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge base를 활용해서 RAG Chatbot Application 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 애플리케이션은 Amazon Bedrock Knowledge base를 활용한 최신 생성형 AI 기능을 시연하기 위해 설계되었습니다.\n",
    "\n",
    "**Amazon Bedrock Agent**: 지능형 챗봇 기능으로, 사용자의 질문 특성에 따라 GraphRAG와 VectorRAG 검색 방식을 자동으로 선택하여 최적의 응답을 제공합니다.  \n",
    "                          GraphRAG는 엔티티 간의 관계와 연결 구조를 분석하는 데 특화되어 있으며, VectorRAG는 의미적 유사성에 기반한 일반 정보 검색에 적합합니다.\n",
    "\n",
    "**Amazon Bedrock Knowledge Base**: 일반 Vector RAG와 Graph RAG 접근 방식의 응답을 나란히 비교할 수 있는 기능을 제공합니다.  \n",
    "                          이를 통해 사용자는 동일한 질문에 대해 두 검색 방식이 어떻게 다른 결과를 도출하는지 직접 확인할 수 있습니다.\n",
    "\n",
    "이 애플리케이션은 Streamlit을 기반으로 개발되어 직관적인 웹 인터페이스를 제공하며, AWS의 Bedrock 서비스를 활용하여 Claude 3.7 Sonnet과 같은 최신 대규모 언어 모델(LLM)의 강력한 기능을 활용합니다.  \n",
    "사용자는 사이드바에서 원하는 데모를 선택하고, 실시간으로 AI의 응답을 확인할 수 있습니다.\n",
    "\n",
    "이 데모는 기업이 Amazon Bedrock을 통해 어떻게 지식 기반 AI 솔루션을 구축하고, 복잡한 질의에 대해 더 정확하고 관련성 높은 응답을 제공할 수 있는지 보여주는 실용적인 예제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 챗봇 실행 파일 생성\n",
    "Amazon Bedrock Agent와 Knowledge base로 구성된 RAG 애플리케이션을 Streamlit UI 기반의 챗봇으로 테스트 해볼 수 있도록 구현.\n",
    "\n",
    "- 준비 사항\n",
    "    - Claude Sonnet 3.5 v2 모델 & Claude instant v1 모델 활성화 필요\n",
    "    - 아래 코드에서 **AGENT_ID, VECTOR_RAG_KB_ID, GRAPH_RAG_KB_ID** 항목을 현재 구성한 정보로 변경 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../app.py\n",
    "\n",
    "import streamlit as st\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from botocore.client import Config\n",
    "\n",
    "# AWS 설정 변수\n",
    "AWS_REGION = \"us-west-2\"\n",
    "AGENT_ALIAS_ID = \"TSTALIASID\"\n",
    "\n",
    "AGENT_ID = \"AR14QVDQII\" #agent id로 수정\n",
    "VECTOR_RAG_KB_ID = \"VXVR4W9Y2O\" #Vector KB ID로 수정\n",
    "GRAPH_RAG_KB_ID = \"DBXNEKHXD4\" #RAG KB ID로 수정\n",
    "\n",
    "# 페이지 설정\n",
    "st.set_page_config(page_title=\"Amazon Bedrock Demos\", layout=\"wide\")\n",
    "\n",
    "# 세션 상태 초기화\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"regular_messages\" not in st.session_state:\n",
    "    st.session_state.regular_messages = []\n",
    "\n",
    "if \"graph_messages\" not in st.session_state:\n",
    "    st.session_state.graph_messages = []\n",
    "\n",
    "# 사이드바에 라디오 버튼 생성\n",
    "with st.sidebar:\n",
    "    st.header(\"데모 선택\")\n",
    "    selected_demo = st.radio(\n",
    "        \"사용할 데모를 선택하세요:\",\n",
    "        [\"Amazon Bedrock Agent\", \"Amazon Bedrock Knowledge Base\"]\n",
    "    )\n",
    "    \n",
    "    # 선택에 따라 추가 정보 표시\n",
    "    if selected_demo == \"Amazon Bedrock Agent\":\n",
    "        st.header(\"Agent 정보\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Agent ID**: {AGENT_ID}\")\n",
    "        st.markdown(f\"**세션 ID**: {st.session_state.session_id}\")\n",
    "        st.markdown(f\"**현재 시간**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        st.header(\"Knowledge Base 정보\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Knowledge Base ID**: {VECTOR_RAG_KB_ID} (Vector), {GRAPH_RAG_KB_ID} (Graph)\")\n",
    "        st.markdown(f\"**현재 시간**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if st.button(\"새 대화 시작\"):\n",
    "        st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "# Amazon Bedrock Agent 기능\n",
    "def run_agent_chatbot():\n",
    "    # Bedrock 클라이언트 초기화\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Semantic 검색 유형 판별 함수\n",
    "    def determine_search_type_semantic(query):\n",
    "        prompt = f\"\"\"\n",
    "        당신은 질문의 특성을 분석하여 적절한 검색 방식을 추천하는 AI입니다.\n",
    "        \n",
    "        다음은 두 가지 검색 방식입니다:\n",
    "        1. GraphRAG: 엔티티 간의 관계, 연결 구조, 네트워크 분석, 경로 탐색, 지식 그래프 패턴 등 관계형 데이터에 적합합니다.\n",
    "        2. VectorRAG: 단순 정보 검색, 키워드 기반 질의, 문서 내용 요약, 유사 개념 탐색 등 의미적 유사성에 기반한 검색에 적합합니다.\n",
    "        \n",
    "        다음 질문을 분석하고, \"GraphRAG\" 또는 \"VectorRAG\" 중 더 적합한 검색 방식을 결정한 후 아래 형식으로 응답해주세요:\n",
    "        \n",
    "        검색방식: [GraphRAG 또는 VectorRAG] \\n\n",
    "        근거: [이 검색 방식을 선택한 이유를 한 문장으로 설명]\n",
    "        \n",
    "        질문: \"{query}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Amazon Bedrock의 Claude 모델을 사용\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=\"anthropic.claude-instant-v1\",\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 200,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            response_text = response_body[\"completion\"].strip()\n",
    "            \n",
    "            # 응답에서 검색 방식과 근거 추출\n",
    "            lines = response_text.split('\\n')\n",
    "            search_type = \"VectorRAG\"  # 기본값\n",
    "            reason = \"기본 검색 방식입니다.\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith(\"검색방식:\"):\n",
    "                    search_type_text = line.replace(\"검색방식:\", \"\").strip()\n",
    "                    if \"GraphRAG\" in search_type_text:\n",
    "                        search_type = \"GraphRAG\"\n",
    "                    else:\n",
    "                        search_type = \"VectorRAG\"\n",
    "                elif line.startswith(\"근거:\"):\n",
    "                    reason = line.replace(\"근거:\", \"\").strip()\n",
    "            \n",
    "            return search_type, reason\n",
    "        except Exception as e:\n",
    "            st.error(f\"검색 유형 결정 중 오류 발생: {str(e)}\")\n",
    "            return \"VectorRAG\", \"오류 발생으로 기본 검색 방식을 사용합니다.\"\n",
    "\n",
    "    # Bedrock Agent에 쿼리 전송 함수\n",
    "    def query_bedrock_agent(query, search_type, reason):\n",
    "        try:\n",
    "            response = bedrock_agent_runtime.invoke_agent(\n",
    "                agentId=AGENT_ID,\n",
    "                agentAliasId=AGENT_ALIAS_ID,\n",
    "                sessionId=st.session_state.session_id,\n",
    "                inputText=query,\n",
    "                enableTrace=False\n",
    "            )\n",
    "            \n",
    "            # 응답 처리\n",
    "            final_response = \"\"\n",
    "            for event in response['completion']:\n",
    "                if 'chunk' in event:\n",
    "                    try:\n",
    "                        chunk_data = json.loads(event['chunk']['bytes'].decode('utf-8'))\n",
    "                        if 'content' in chunk_data:\n",
    "                            final_response += chunk_data['content']\n",
    "                    except json.JSONDecodeError:\n",
    "                        content = event['chunk']['bytes'].decode('utf-8')\n",
    "                        final_response += content\n",
    "            \n",
    "            # 검색 유형과 근거 표시\n",
    "            final_response += \"\\n\\n---\"\n",
    "            final_response += f\"\\n\\n검색 방식: {search_type}\"\n",
    "            final_response += f\"\\n\\n선택 근거: {reason}\"\n",
    "            final_response += \"\\n\\n---\"\n",
    "            \n",
    "            return final_response\n",
    "        except Exception as e:\n",
    "            return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "    # Streamlit UI 설정\n",
    "    st.title(\"Amazon Bedrock Agent\")\n",
    "    st.markdown(\"일반 Vector RAG와 Graph RAG를 활용한 Agentic 챗봇입니다.\")\n",
    "\n",
    "    # 채팅 기록 표시\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # 사용자 입력 처리\n",
    "    if prompt := st.chat_input(\"질문을 입력하세요...\"):\n",
    "        # 사용자 메시지 표시\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # 의미론적 검색 유형 결정\n",
    "        with st.spinner(\"검색 방식 분석 중...\"):\n",
    "            search_type, reason = determine_search_type_semantic(prompt)\n",
    "        \n",
    "        # 로딩 표시\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"🤔 생각 중...\")\n",
    "            \n",
    "            # Bedrock Agent에 쿼리 전송\n",
    "            response = query_bedrock_agent(prompt, search_type, reason)\n",
    "            \n",
    "            # 응답 표시\n",
    "            message_placeholder.markdown(response)\n",
    "        \n",
    "        # 채팅 기록에 응답 추가\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Amazon Bedrock Knowledge Base 기능\n",
    "def run_knowledge_base_demo():\n",
    "    # Bedrock 클라이언트 초기화\n",
    "    bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\", \n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "\n",
    "    # 미리 정의된 프롬프트\n",
    "    predefined_prompts = [\n",
    "        \"Amazon은 운영 비용의 증가가 다른 재무 지표들에 어떤 영향을 미쳤나요?\",\n",
    "        \"Amazon의 총 순매출은 시간이 지남에 따라 어떻게 변화했나요?\",\n",
    "        \"Amazon의 온라인 소매 서비스 매출은 분기별로 어떻게 변동했나요?\"\n",
    "    ]\n",
    "\n",
    "    # 페이지 헤더\n",
    "    st.title(\"Amazon Bedrock Knowledge Base\")\n",
    "    st.markdown(\"일반 Vector RAG와 Graph RAG 접근 방식 간의 응답 비교\")\n",
    "\n",
    "    # Knowledge Base에서 컨텍스트 검색 함수\n",
    "    def retrieve_from_knowledge_base(query, kb_id, number_of_results=3):\n",
    "        try:\n",
    "            # KB ID에 따라 검색 유형 결정\n",
    "            search_type = \"SEMANTIC\" if kb_id == GRAPH_RAG_KB_ID else \"HYBRID\"\n",
    "            \n",
    "            # Knowledge Base에서 컨텍스트 검색\n",
    "            response = bedrock_agent_runtime.retrieve(\n",
    "                retrievalQuery={\n",
    "                    'text': query\n",
    "                },\n",
    "                knowledgeBaseId=kb_id,\n",
    "                retrievalConfiguration={\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'numberOfResults': number_of_results,\n",
    "                        'overrideSearchType': search_type\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # 결과에서 컨텍스트 추출\n",
    "            contexts = []\n",
    "            if 'retrievalResults' in response:\n",
    "                for result in response['retrievalResults']:\n",
    "                    if 'content' in result and 'text' in result['content']:\n",
    "                        contexts.append(result['content']['text'])\n",
    "            \n",
    "            return contexts\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # Knowledge Base 쿼리 함수\n",
    "    def query_knowledge_base(query, kb_id):\n",
    "        try:\n",
    "            # Knowledge Base에서 관련 컨텍스트 검색\n",
    "            contexts = retrieve_from_knowledge_base(query, kb_id)\n",
    "            \n",
    "            if isinstance(contexts, str) and contexts.startswith(\"Error\"):\n",
    "                return contexts  # 오류 메시지 반환\n",
    "            \n",
    "            # 검색된 컨텍스트로 프롬프트 포맷팅\n",
    "            prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions by using fact based when possible.\n",
    "You're a helpful assistant who loves to respond in Korean.\n",
    "Use the following pieces of information to provide a detailed answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "            # Claude 3.7 Sonnet 사용\n",
    "            model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\" \n",
    "            \n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 8192,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0\n",
    "                }),\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            # 응답 추출\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            response_text = response_body.get('content')[0]['text']\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error querying Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # 응답을 위한 두 개의 열 생성\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # 채팅 기록 표시\n",
    "    with col1:\n",
    "        st.subheader(\"Vector RAG\")\n",
    "        for message in st.session_state.regular_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Graph RAG\")\n",
    "        for message in st.session_state.graph_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    # 미리 정의된 프롬프트에서 사용자 선택\n",
    "    selected_prompt_index = st.selectbox(\"질문을 선택하세요:\", options=range(len(predefined_prompts)), format_func=lambda i: predefined_prompts[i])\n",
    "    \n",
    "    # 선택한 프롬프트 제출 버튼\n",
    "    if st.button(\"질문 제출\"):\n",
    "        # 선택한 프롬프트 가져오기 - 이 부분이 수정되었습니다\n",
    "        user_query = predefined_prompts[selected_prompt_index]\n",
    "        \n",
    "        # 이전 메시지 지우기\n",
    "        st.session_state.regular_messages = []\n",
    "        st.session_state.graph_messages = []\n",
    "        \n",
    "        # 두 채팅 기록에 사용자 메시지 추가\n",
    "        st.session_state.regular_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state.graph_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        # 두 열에 사용자 메시지 표시\n",
    "        with col1:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        with col2:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        \n",
    "        # 두 지식 베이스 쿼리\n",
    "        regular_response = query_knowledge_base(user_query, VECTOR_RAG_KB_ID)\n",
    "        graph_response = query_knowledge_base(user_query, GRAPH_RAG_KB_ID)\n",
    "        \n",
    "        # 채팅 기록에 어시스턴트 응답 추가\n",
    "        st.session_state.regular_messages.append({\"role\": \"assistant\", \"content\": regular_response})\n",
    "        st.session_state.graph_messages.append({\"role\": \"assistant\", \"content\": graph_response})\n",
    "        \n",
    "        # 어시스턴트 응답 표시\n",
    "        with col1:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(regular_response)\n",
    "        \n",
    "        with col2:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(graph_response)\n",
    "        \n",
    "        # UI 업데이트를 위한 재실행\n",
    "        st.rerun()\n",
    "\n",
    "# 라디오 버튼 선택에 따라 선택된 데모 실행\n",
    "if selected_demo == \"Amazon Bedrock Agent\":\n",
    "    run_agent_chatbot()\n",
    "else:\n",
    "    run_knowledge_base_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 챗봇 실행을 위한 requirements 설치 및 실행\n",
    "\n",
    "- 아래 코드를 실행 하고 현재 SageMaker 노트북이 열려있는 웹 브라우저의 URL을 복사하고 URL 마지막 부분을 아래와 같이 수정 후 실행 가능\n",
    "\n",
    "```python\n",
    "https://rag-test.studio.us-west-2.sagemaker.aws/jupyterlab/default/[추가]proxy/8080/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앱 실행\n",
    "!streamlit run ../../app.py --server.port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Option - 코드 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트 및 설정\n",
    "필요한 라이브러리를 임포트하고 AWS 리소스 ID와 리전 설정을 위한 변수들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from botocore.client import Config\n",
    "\n",
    "# AWS 설정 변수\n",
    "AWS_REGION = \"us-west-2\" #사용중인 Region으로 변경\n",
    "AGENT_ID = \"AR14QVDQII\" #Agent_ID로 수정\n",
    "AGENT_ALIAS_ID = \"TSTALIASID\" #Agent alias id는 현재 값으로 유지 \"TSTALIASID\"\n",
    "VECTOR_RAG_KB_ID = \"VXVR4W9Y2O\" #VectorRAG KB ID로 수정\n",
    "GRAPH_RAG_KB_ID = \"DBXNEKHXD4\" #GraphRAG KB ID로 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 페이지 설정 및 세션 상태 초기화\n",
    "Streamlit 페이지를 설정하고 대화 세션을 관리하기 위한 세션 상태 변수들을 초기화합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 설정\n",
    "st.set_page_config(page_title=\"Amazon Bedrock RAG Demos\", layout=\"wide\")\n",
    "\n",
    "# 세션 상태 초기화\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"regular_messages\" not in st.session_state:\n",
    "    st.session_state.regular_messages = []\n",
    "\n",
    "if \"graph_messages\" not in st.session_state:\n",
    "    st.session_state.graph_messages = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 사이드바 UI 구성\n",
    "사이드바에 데모 선택 라디오 버튼과 관련 정보를 표시합니다. 선택된 데모에 따라 다른 정보를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이드바에 라디오 버튼 생성\n",
    "with st.sidebar:\n",
    "    st.header(\"데모 선택\")\n",
    "    selected_demo = st.radio(\n",
    "        \"사용할 데모를 선택하세요:\",\n",
    "        [\"Amazon Bedrock Agent\", \"Amazon Bedrock Knowledge Base\"]\n",
    "    )\n",
    "    \n",
    "    # 선택에 따라 추가 정보 표시\n",
    "    if selected_demo == \"Amazon Bedrock Agent\":\n",
    "        st.header(\"Agent 정보\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Agent ID**: {AGENT_ID}\")\n",
    "        st.markdown(f\"**세션 ID**: {st.session_state.session_id}\")\n",
    "        st.markdown(f\"**현재 시간**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        st.header(\"Knowledge Base 정보\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Knowledge Base ID**: {VECTOR_RAG_KB_ID} (Vector), {GRAPH_RAG_KB_ID} (Graph)\")\n",
    "        st.markdown(f\"**현재 시간**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if st.button(\"새 대화 시작\"):\n",
    "        st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Amazon Bedrock Agent RAG 챗봇\n",
    "Amazon Bedrock Agent를 활용한 챗봇 기능으로, 질문 특성에 따라 GraphRAG와 VectorRAG 검색 방식을 자동으로 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_chatbot():\n",
    "    # Bedrock 클라이언트 초기화\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Semantic 검색 유형 판별 함수\n",
    "    def determine_search_type_semantic(query):\n",
    "        prompt = f\"\"\"\n",
    "        당신은 질문의 특성을 분석하여 적절한 검색 방식을 추천하는 AI입니다.\n",
    "        \n",
    "        다음은 두 가지 검색 방식입니다:\n",
    "        1. GraphRAG: 엔티티 간의 관계, 연결 구조, 네트워크 분석, 경로 탐색, 지식 그래프 패턴 등 관계형 데이터에 적합합니다.\n",
    "        2. VectorRAG: 단순 정보 검색, 키워드 기반 질의, 문서 내용 요약, 유사 개념 탐색 등 의미적 유사성에 기반한 검색에 적합합니다.\n",
    "        \n",
    "        다음 질문을 분석하고, \"GraphRAG\" 또는 \"VectorRAG\" 중 더 적합한 검색 방식을 결정한 후 아래 형식으로 응답해주세요:\n",
    "        \n",
    "        검색방식: [GraphRAG 또는 VectorRAG] \\n\n",
    "        근거: [이 검색 방식을 선택한 이유를 한 문장으로 설명]\n",
    "        \n",
    "        질문: \"{query}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Amazon Bedrock의 Claude 모델을 사용\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=\"anthropic.claude-instant-v1\",\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 200,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            response_text = response_body[\"completion\"].strip()\n",
    "            \n",
    "            # 응답에서 검색 방식과 근거 추출\n",
    "            lines = response_text.split('\\n')\n",
    "            search_type = \"VectorRAG\"  # 기본값\n",
    "            reason = \"기본 검색 방식입니다.\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith(\"검색방식:\"):\n",
    "                    search_type_text = line.replace(\"검색방식:\", \"\").strip()\n",
    "                    if \"GraphRAG\" in search_type_text:\n",
    "                        search_type = \"GraphRAG\"\n",
    "                    else:\n",
    "                        search_type = \"VectorRAG\"\n",
    "                elif line.startswith(\"근거:\"):\n",
    "                    reason = line.replace(\"근거:\", \"\").strip()\n",
    "            \n",
    "            return search_type, reason\n",
    "        except Exception as e:\n",
    "            st.error(f\"검색 유형 결정 중 오류 발생: {str(e)}\")\n",
    "            return \"VectorRAG\", \"오류 발생으로 기본 검색 방식을 사용합니다.\"\n",
    "\n",
    "    # Bedrock Agent에 쿼리 전송 함수\n",
    "    def query_bedrock_agent(query, search_type, reason):\n",
    "        try:\n",
    "            response = bedrock_agent_runtime.invoke_agent(\n",
    "                agentId=AGENT_ID,\n",
    "                agentAliasId=AGENT_ALIAS_ID,\n",
    "                sessionId=st.session_state.session_id,\n",
    "                inputText=query,\n",
    "                enableTrace=False\n",
    "            )\n",
    "            \n",
    "            # 응답 처리\n",
    "            final_response = \"\"\n",
    "            for event in response['completion']:\n",
    "                if 'chunk' in event:\n",
    "                    try:\n",
    "                        chunk_data = json.loads(event['chunk']['bytes'].decode('utf-8'))\n",
    "                        if 'content' in chunk_data:\n",
    "                            final_response += chunk_data['content']\n",
    "                    except json.JSONDecodeError:\n",
    "                        content = event['chunk']['bytes'].decode('utf-8')\n",
    "                        final_response += content\n",
    "            \n",
    "            # 검색 유형과 근거 표시\n",
    "            final_response += \"\\n\\n---\"\n",
    "            final_response += f\"\\n\\n검색 방식: {search_type}\"\n",
    "            final_response += f\"\\n\\n선택 근거: {reason}\"\n",
    "            final_response += \"\\n\\n---\"\n",
    "            \n",
    "            return final_response\n",
    "        except Exception as e:\n",
    "            return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "    # Streamlit UI 설정\n",
    "    st.title(\"Amazon Bedrock Agent\")\n",
    "    st.markdown(\"GraphRAG와 VectorRAG를 활용한 지능형 챗봇입니다.\")\n",
    "\n",
    "    # 채팅 기록 표시\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # 사용자 입력 처리\n",
    "    if prompt := st.chat_input(\"질문을 입력하세요...\"):\n",
    "        # 사용자 메시지 표시\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # 의미론적 검색 유형 결정\n",
    "        with st.spinner(\"검색 방식 분석 중...\"):\n",
    "            search_type, reason = determine_search_type_semantic(prompt)\n",
    "        \n",
    "        # 로딩 표시\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"🤔 생각 중...\")\n",
    "            \n",
    "            # Bedrock Agent에 쿼리 전송\n",
    "            response = query_bedrock_agent(prompt, search_type, reason)\n",
    "            \n",
    "            # 응답 표시\n",
    "            message_placeholder.markdown(response)\n",
    "        \n",
    "        # 채팅 기록에 응답 추가\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Amazon Bedrock Knowledge Base 비교 챗봇\n",
    "Vector RAG와 Graph RAG 접근 방식의 응답을 나란히 비교할 수 있는 기능을 제공합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knowledge_base_demo():\n",
    "    # Bedrock 클라이언트 초기화\n",
    "    bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\", \n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "\n",
    "    # 미리 정의된 프롬프트\n",
    "    predefined_prompts = [\n",
    "        \"Amazon은 운영 비용의 증가가 다른 재무 지표들에 어떤 영향을 미쳤나요?\",\n",
    "        \"Amazon의 총 순매출은 시간이 지남에 따라 어떻게 변화했나요?\",\n",
    "        \"Amazon의 온라인 소매 서비스 매출은 분기별로 어떻게 변동했나요?\"\n",
    "    ]\n",
    "\n",
    "    # 페이지 헤더\n",
    "    st.title(\"Amazon Bedrock Knowledge Base\")\n",
    "    st.markdown(\"일반 Vector RAG와 Graph RAG 접근 방식 간의 응답 비교\")\n",
    "\n",
    "    # Knowledge Base에서 컨텍스트 검색 함수\n",
    "    def retrieve_from_knowledge_base(query, kb_id, number_of_results=3):\n",
    "        try:\n",
    "            # KB ID에 따라 검색 유형 결정\n",
    "            search_type = \"SEMANTIC\" if kb_id == GRAPH_RAG_KB_ID else \"HYBRID\"\n",
    "            \n",
    "            # Knowledge Base에서 컨텍스트 검색\n",
    "            response = bedrock_agent_runtime.retrieve(\n",
    "                retrievalQuery={\n",
    "                    'text': query\n",
    "                },\n",
    "                knowledgeBaseId=kb_id,\n",
    "                retrievalConfiguration={\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'numberOfResults': number_of_results,\n",
    "                        'overrideSearchType': search_type\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # 결과에서 컨텍스트 추출\n",
    "            contexts = []\n",
    "            if 'retrievalResults' in response:\n",
    "                for result in response['retrievalResults']:\n",
    "                    if 'content' in result and 'text' in result['content']:\n",
    "                        contexts.append(result['content']['text'])\n",
    "            \n",
    "            return contexts\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # Knowledge Base 쿼리 함수\n",
    "    def query_knowledge_base(query, kb_id):\n",
    "        try:\n",
    "            # Knowledge Base에서 관련 컨텍스트 검색\n",
    "            contexts = retrieve_from_knowledge_base(query, kb_id)\n",
    "            \n",
    "            if isinstance(contexts, str) and contexts.startswith(\"Error\"):\n",
    "                return contexts  # 오류 메시지 반환\n",
    "            \n",
    "            # 검색된 컨텍스트로 프롬프트 포맷팅\n",
    "            prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions by using fact based when possible.\n",
    "You're a helpful assistant who loves to respond in Korean.\n",
    "Use the following pieces of information to provide a detailed answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "            # Claude 5 Sonnet 사용\n",
    "            model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\" \n",
    "            \n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 8192,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0\n",
    "                }),\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            # 응답 추출\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            response_text = response_body.get('content')[0]['text']\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error querying Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # 응답을 위한 두 개의 열 생성\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # 채팅 기록 표시\n",
    "    with col1:\n",
    "        st.subheader(\"Vector RAG\")\n",
    "        for message in st.session_state.regular_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Graph RAG\")\n",
    "        for message in st.session_state.graph_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    # 미리 정의된 프롬프트에서 사용자 선택\n",
    "    selected_prompt_index = st.selectbox(\"질문을 선택하세요:\", options=range(len(predefined_prompts)), format_func=lambda i: predefined_prompts[i])\n",
    "    \n",
    "    # 선택한 프롬프트 제출 버튼\n",
    "    if st.button(\"질문 제출\"):\n",
    "        user_query = predefined_prompts[selected_prompt_index]\n",
    "        \n",
    "        # 이전 메시지 지우기\n",
    "        st.session_state.regular_messages = []\n",
    "        st.session_state.graph_messages = []\n",
    "        \n",
    "        # 두 채팅 기록에 사용자 메시지 추가\n",
    "        st.session_state.regular_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state.graph_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        # 두 열에 사용자 메시지 표시\n",
    "        with col1:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        with col2:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        \n",
    "        # 두 지식 베이스 쿼리\n",
    "        regular_response = query_knowledge_base(user_query, VECTOR_RAG_KB_ID)\n",
    "        graph_response = query_knowledge_base(user_query, GRAPH_RAG_KB_ID)\n",
    "        \n",
    "        # 채팅 기록에 어시스턴트 응답 추가\n",
    "        st.session_state.regular_messages.append({\"role\": \"assistant\", \"content\": regular_response})\n",
    "        st.session_state.graph_messages.append({\"role\": \"assistant\", \"content\": graph_response})\n",
    "        \n",
    "        # 어시스턴트 응답 표시\n",
    "        with col1:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(regular_response)\n",
    "        \n",
    "        with col2:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(graph_response)\n",
    "        \n",
    "        # UI 업데이트를 위한 재실행\n",
    "        st.rerun()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 챗봇 실행 메인 로직\n",
    "사용자가 선택한 데모에 따라 해당하는 기능을 실행하는 메인 로직입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라디오 버튼 선택에 따라 선택된 데모 실행\n",
    "if selected_demo == \"Amazon Bedrock Agent\":\n",
    "    run_agent_chatbot()\n",
    "else:\n",
    "    run_knowledge_base_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 챗봇 실행 파일 생성\n",
    "Amazon Bedrock Agent와 Knowledge base로 구성된 RAG 애플리케이션을 Streamlit UI 기반의 챗봇으로 테스트 해볼 수 있도록 구현.\n",
    "\n",
    "- 준비 사항\n",
    "    - Claude Sonnet 3.5 v2 모델 & Claude instant v1 모델 활성화 필요\n",
    "    - 아래 코드에서 **AGENT_ID, VECTOR_RAG_KB_ID, GRAPH_RAG_KB_ID** 항목을 현재 구성한 정보로 변경 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../app.py\n",
    "\n",
    "import streamlit as st\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from botocore.client import Config\n",
    "\n",
    "# AWS 설정 변수\n",
    "AWS_REGION = \"us-west-2\"\n",
    "AGENT_ALIAS_ID = \"TSTALIASID\"\n",
    "\n",
    "AGENT_ID = \"AR14QVDQII\" #agent id로 수정\n",
    "VECTOR_RAG_KB_ID = \"VXVR4W9Y2O\" #Vector KB ID로 수정\n",
    "GRAPH_RAG_KB_ID = \"DBXNEKHXD4\" #RAG KB ID로 수정\n",
    "\n",
    "# 페이지 설정\n",
    "st.set_page_config(page_title=\"Amazon Bedrock Demos\", layout=\"wide\")\n",
    "\n",
    "# 세션 상태 초기화\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"regular_messages\" not in st.session_state:\n",
    "    st.session_state.regular_messages = []\n",
    "\n",
    "if \"graph_messages\" not in st.session_state:\n",
    "    st.session_state.graph_messages = []\n",
    "\n",
    "# 사이드바에 라디오 버튼 생성\n",
    "with st.sidebar:\n",
    "    st.header(\"데모 선택\")\n",
    "    selected_demo = st.radio(\n",
    "        \"사용할 데모를 선택하세요:\",\n",
    "        [\"Amazon Bedrock Agent\", \"Amazon Bedrock Knowledge Base\"]\n",
    "    )\n",
    "    \n",
    "    # 선택에 따라 추가 정보 표시\n",
    "    if selected_demo == \"Amazon Bedrock Agent\":\n",
    "        st.header(\"Agent 정보\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Agent ID**: {AGENT_ID}\")\n",
    "        st.markdown(f\"**세션 ID**: {st.session_state.session_id}\")\n",
    "        st.markdown(f\"**현재 시간**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        st.header(\"Knowledge Base 정보\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Knowledge Base ID**: {VECTOR_RAG_KB_ID} (Vector), {GRAPH_RAG_KB_ID} (Graph)\")\n",
    "        st.markdown(f\"**현재 시간**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if st.button(\"새 대화 시작\"):\n",
    "        st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "# Amazon Bedrock Agent 기능\n",
    "def run_agent_chatbot():\n",
    "    # Bedrock 클라이언트 초기화\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Semantic 검색 유형 판별 함수\n",
    "    def determine_search_type_semantic(query):\n",
    "        prompt = f\"\"\"\n",
    "        당신은 질문의 특성을 분석하여 적절한 검색 방식을 추천하는 AI입니다.\n",
    "        \n",
    "        다음은 두 가지 검색 방식입니다:\n",
    "        1. GraphRAG: 엔티티 간의 관계, 연결 구조, 네트워크 분석, 경로 탐색, 지식 그래프 패턴 등 관계형 데이터에 적합합니다.\n",
    "        2. VectorRAG: 단순 정보 검색, 키워드 기반 질의, 문서 내용 요약, 유사 개념 탐색 등 의미적 유사성에 기반한 검색에 적합합니다.\n",
    "        \n",
    "        다음 질문을 분석하고, \"GraphRAG\" 또는 \"VectorRAG\" 중 더 적합한 검색 방식을 결정한 후 아래 형식으로 응답해주세요:\n",
    "        \n",
    "        검색방식: [GraphRAG 또는 VectorRAG] \\n\n",
    "        근거: [이 검색 방식을 선택한 이유를 한 문장으로 설명]\n",
    "        \n",
    "        질문: \"{query}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Amazon Bedrock의 Claude 모델을 사용\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=\"anthropic.claude-instant-v1\",\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 200,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            response_text = response_body[\"completion\"].strip()\n",
    "            \n",
    "            # 응답에서 검색 방식과 근거 추출\n",
    "            lines = response_text.split('\\n')\n",
    "            search_type = \"VectorRAG\"  # 기본값\n",
    "            reason = \"기본 검색 방식입니다.\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith(\"검색방식:\"):\n",
    "                    search_type_text = line.replace(\"검색방식:\", \"\").strip()\n",
    "                    if \"GraphRAG\" in search_type_text:\n",
    "                        search_type = \"GraphRAG\"\n",
    "                    else:\n",
    "                        search_type = \"VectorRAG\"\n",
    "                elif line.startswith(\"근거:\"):\n",
    "                    reason = line.replace(\"근거:\", \"\").strip()\n",
    "            \n",
    "            return search_type, reason\n",
    "        except Exception as e:\n",
    "            st.error(f\"검색 유형 결정 중 오류 발생: {str(e)}\")\n",
    "            return \"VectorRAG\", \"오류 발생으로 기본 검색 방식을 사용합니다.\"\n",
    "\n",
    "    # Bedrock Agent에 쿼리 전송 함수\n",
    "    def query_bedrock_agent(query, search_type, reason):\n",
    "        try:\n",
    "            response = bedrock_agent_runtime.invoke_agent(\n",
    "                agentId=AGENT_ID,\n",
    "                agentAliasId=AGENT_ALIAS_ID,\n",
    "                sessionId=st.session_state.session_id,\n",
    "                inputText=query,\n",
    "                enableTrace=False\n",
    "            )\n",
    "            \n",
    "            # 응답 처리\n",
    "            final_response = \"\"\n",
    "            for event in response['completion']:\n",
    "                if 'chunk' in event:\n",
    "                    try:\n",
    "                        chunk_data = json.loads(event['chunk']['bytes'].decode('utf-8'))\n",
    "                        if 'content' in chunk_data:\n",
    "                            final_response += chunk_data['content']\n",
    "                    except json.JSONDecodeError:\n",
    "                        content = event['chunk']['bytes'].decode('utf-8')\n",
    "                        final_response += content\n",
    "            \n",
    "            # 검색 유형과 근거 표시\n",
    "            final_response += \"\\n\\n---\"\n",
    "            final_response += f\"\\n\\n검색 방식: {search_type}\"\n",
    "            final_response += f\"\\n\\n선택 근거: {reason}\"\n",
    "            final_response += \"\\n\\n---\"\n",
    "            \n",
    "            return final_response\n",
    "        except Exception as e:\n",
    "            return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "    # Streamlit UI 설정\n",
    "    st.title(\"Amazon Bedrock Agent\")\n",
    "    st.markdown(\"일반 Vector RAG와 Graph RAG를 활용한 Agentic 챗봇입니다.\")\n",
    "\n",
    "    # 채팅 기록 표시\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # 사용자 입력 처리\n",
    "    if prompt := st.chat_input(\"질문을 입력하세요...\"):\n",
    "        # 사용자 메시지 표시\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # 의미론적 검색 유형 결정\n",
    "        with st.spinner(\"검색 방식 분석 중...\"):\n",
    "            search_type, reason = determine_search_type_semantic(prompt)\n",
    "        \n",
    "        # 로딩 표시\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"🤔 생각 중...\")\n",
    "            \n",
    "            # Bedrock Agent에 쿼리 전송\n",
    "            response = query_bedrock_agent(prompt, search_type, reason)\n",
    "            \n",
    "            # 응답 표시\n",
    "            message_placeholder.markdown(response)\n",
    "        \n",
    "        # 채팅 기록에 응답 추가\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Amazon Bedrock Knowledge Base 기능\n",
    "def run_knowledge_base_demo():\n",
    "    # Bedrock 클라이언트 초기화\n",
    "    bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\", \n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "\n",
    "    # 미리 정의된 프롬프트\n",
    "    predefined_prompts = [\n",
    "        \"Amazon은 운영 비용의 증가가 다른 재무 지표들에 어떤 영향을 미쳤나요?\",\n",
    "        \"Amazon의 총 순매출은 시간이 지남에 따라 어떻게 변화했나요?\",\n",
    "        \"Amazon의 온라인 소매 서비스 매출은 분기별로 어떻게 변동했나요?\"\n",
    "    ]\n",
    "\n",
    "    # 페이지 헤더\n",
    "    st.title(\"Amazon Bedrock Knowledge Base\")\n",
    "    st.markdown(\"일반 Vector RAG와 Graph RAG 접근 방식 간의 응답 비교\")\n",
    "\n",
    "    # Knowledge Base에서 컨텍스트 검색 함수\n",
    "    def retrieve_from_knowledge_base(query, kb_id, number_of_results=3):\n",
    "        try:\n",
    "            # KB ID에 따라 검색 유형 결정\n",
    "            search_type = \"SEMANTIC\" if kb_id == GRAPH_RAG_KB_ID else \"HYBRID\"\n",
    "            \n",
    "            # Knowledge Base에서 컨텍스트 검색\n",
    "            response = bedrock_agent_runtime.retrieve(\n",
    "                retrievalQuery={\n",
    "                    'text': query\n",
    "                },\n",
    "                knowledgeBaseId=kb_id,\n",
    "                retrievalConfiguration={\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'numberOfResults': number_of_results,\n",
    "                        'overrideSearchType': search_type\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # 결과에서 컨텍스트 추출\n",
    "            contexts = []\n",
    "            if 'retrievalResults' in response:\n",
    "                for result in response['retrievalResults']:\n",
    "                    if 'content' in result and 'text' in result['content']:\n",
    "                        contexts.append(result['content']['text'])\n",
    "            \n",
    "            return contexts\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # Knowledge Base 쿼리 함수\n",
    "    def query_knowledge_base(query, kb_id):\n",
    "        try:\n",
    "            # Knowledge Base에서 관련 컨텍스트 검색\n",
    "            contexts = retrieve_from_knowledge_base(query, kb_id)\n",
    "            \n",
    "            if isinstance(contexts, str) and contexts.startswith(\"Error\"):\n",
    "                return contexts  # 오류 메시지 반환\n",
    "            \n",
    "            # 검색된 컨텍스트로 프롬프트 포맷팅\n",
    "            prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions by using fact based when possible.\n",
    "You're a helpful assistant who loves to respond in Korean.\n",
    "Use the following pieces of information to provide a detailed answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "            # Claude 3.5 Sonnet 사용\n",
    "            model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\" \n",
    "            \n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 8192,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0\n",
    "                }),\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            # 응답 추출\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            response_text = response_body.get('content')[0]['text']\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error querying Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # 응답을 위한 두 개의 열 생성\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # 채팅 기록 표시\n",
    "    with col1:\n",
    "        st.subheader(\"Vector RAG\")\n",
    "        for message in st.session_state.regular_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Graph RAG\")\n",
    "        for message in st.session_state.graph_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    # 미리 정의된 프롬프트에서 사용자 선택\n",
    "    selected_prompt_index = st.selectbox(\"질문을 선택하세요:\", options=range(len(predefined_prompts)), format_func=lambda i: predefined_prompts[i])\n",
    "    \n",
    "    # 선택한 프롬프트 제출 버튼\n",
    "    if st.button(\"질문 제출\"):\n",
    "        # 선택한 프롬프트 가져오기 - 이 부분이 수정되었습니다\n",
    "        user_query = predefined_prompts[selected_prompt_index]\n",
    "        \n",
    "        # 이전 메시지 지우기\n",
    "        st.session_state.regular_messages = []\n",
    "        st.session_state.graph_messages = []\n",
    "        \n",
    "        # 두 채팅 기록에 사용자 메시지 추가\n",
    "        st.session_state.regular_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state.graph_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        # 두 열에 사용자 메시지 표시\n",
    "        with col1:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        with col2:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        \n",
    "        # 두 지식 베이스 쿼리\n",
    "        regular_response = query_knowledge_base(user_query, VECTOR_RAG_KB_ID)\n",
    "        graph_response = query_knowledge_base(user_query, GRAPH_RAG_KB_ID)\n",
    "        \n",
    "        # 채팅 기록에 어시스턴트 응답 추가\n",
    "        st.session_state.regular_messages.append({\"role\": \"assistant\", \"content\": regular_response})\n",
    "        st.session_state.graph_messages.append({\"role\": \"assistant\", \"content\": graph_response})\n",
    "        \n",
    "        # 어시스턴트 응답 표시\n",
    "        with col1:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(regular_response)\n",
    "        \n",
    "        with col2:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(graph_response)\n",
    "        \n",
    "        # UI 업데이트를 위한 재실행\n",
    "        st.rerun()\n",
    "\n",
    "# 라디오 버튼 선택에 따라 선택된 데모 실행\n",
    "if selected_demo == \"Amazon Bedrock Agent\":\n",
    "    run_agent_chatbot()\n",
    "else:\n",
    "    run_knowledge_base_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 챗봇 실행을 위한 requirements 설치 및 실행\n",
    "\n",
    "- 아래 코드를 실행 하고 현재 SageMaker 노트북이 열려있는 웹 브라우저의 URL을 복사하고 URL 마지막 부분을 아래와 같이 수정 후 실행 가능\n",
    "    - 예시 - https://rag-test.studio.us-west-2.sagemaker.aws/jupyterlab/**default/proxy/8080/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 앱 실행\n",
    "streamlit run ../app.py --server.port 8080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
