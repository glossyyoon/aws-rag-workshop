{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge baseë¥¼ í™œìš©í•´ì„œ RAG Chatbot Application ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Amazon Bedrock Knowledge baseë¥¼ í™œìš©í•œ ìµœì‹  ìƒì„±í˜• AI ê¸°ëŠ¥ì„ ì‹œì—°í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**Amazon Bedrock Agent**: ì§€ëŠ¥í˜• ì±—ë´‡ ê¸°ëŠ¥ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ íŠ¹ì„±ì— ë”°ë¼ GraphRAGì™€ VectorRAG ê²€ìƒ‰ ë°©ì‹ì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ìµœì ì˜ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.  \n",
    "                          GraphRAGëŠ” ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ì™€ ì—°ê²° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, VectorRAGëŠ” ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ ì¼ë°˜ ì •ë³´ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "**Amazon Bedrock Knowledge Base**: ì¼ë°˜ Vector RAGì™€ Graph RAG ì ‘ê·¼ ë°©ì‹ì˜ ì‘ë‹µì„ ë‚˜ë€íˆ ë¹„êµí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.  \n",
    "                          ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ë‘ ê²€ìƒ‰ ë°©ì‹ì´ ì–´ë–»ê²Œ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ”ì§€ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Streamlitì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì–´ ì§ê´€ì ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë©°, AWSì˜ Bedrock ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ Claude 3.7 Sonnetê³¼ ê°™ì€ ìµœì‹  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ í™œìš©í•©ë‹ˆë‹¤.  \n",
    "ì‚¬ìš©ìëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì›í•˜ëŠ” ë°ëª¨ë¥¼ ì„ íƒí•˜ê³ , ì‹¤ì‹œê°„ìœ¼ë¡œ AIì˜ ì‘ë‹µì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°ëª¨ëŠ” ê¸°ì—…ì´ Amazon Bedrockì„ í†µí•´ ì–´ë–»ê²Œ ì§€ì‹ ê¸°ë°˜ AI ì†”ë£¨ì…˜ì„ êµ¬ì¶•í•˜ê³ , ë³µì¡í•œ ì§ˆì˜ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì‹¤ìš©ì ì¸ ì˜ˆì œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ì±—ë´‡ ì‹¤í–‰ íŒŒì¼ ìƒì„±\n",
    "Amazon Bedrock Agentì™€ Knowledge baseë¡œ êµ¬ì„±ëœ RAG ì• í”Œë¦¬ì¼€ì´ì…˜ì„ Streamlit UI ê¸°ë°˜ì˜ ì±—ë´‡ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í•´ë³¼ ìˆ˜ ìˆë„ë¡ êµ¬í˜„.\n",
    "\n",
    "- ì¤€ë¹„ ì‚¬í•­\n",
    "    - Claude Sonnet 3.5 v2 ëª¨ë¸ & Claude instant v1 ëª¨ë¸ í™œì„±í™” í•„ìš”\n",
    "    - ì•„ë˜ ì½”ë“œì—ì„œ **AGENT_ID, VECTOR_RAG_KB_ID, GRAPH_RAG_KB_ID** í•­ëª©ì„ í˜„ì¬ êµ¬ì„±í•œ ì •ë³´ë¡œ ë³€ê²½ í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../app.py\n",
    "\n",
    "import streamlit as st\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from botocore.client import Config\n",
    "\n",
    "# AWS ì„¤ì • ë³€ìˆ˜\n",
    "AWS_REGION = \"us-west-2\"\n",
    "AGENT_ALIAS_ID = \"TSTALIASID\"\n",
    "\n",
    "AGENT_ID = \"AR14QVDQII\" #agent idë¡œ ìˆ˜ì •\n",
    "VECTOR_RAG_KB_ID = \"VXVR4W9Y2O\" #Vector KB IDë¡œ ìˆ˜ì •\n",
    "GRAPH_RAG_KB_ID = \"DBXNEKHXD4\" #RAG KB IDë¡œ ìˆ˜ì •\n",
    "\n",
    "# í˜ì´ì§€ ì„¤ì •\n",
    "st.set_page_config(page_title=\"Amazon Bedrock Demos\", layout=\"wide\")\n",
    "\n",
    "# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"regular_messages\" not in st.session_state:\n",
    "    st.session_state.regular_messages = []\n",
    "\n",
    "if \"graph_messages\" not in st.session_state:\n",
    "    st.session_state.graph_messages = []\n",
    "\n",
    "# ì‚¬ì´ë“œë°”ì— ë¼ë””ì˜¤ ë²„íŠ¼ ìƒì„±\n",
    "with st.sidebar:\n",
    "    st.header(\"ë°ëª¨ ì„ íƒ\")\n",
    "    selected_demo = st.radio(\n",
    "        \"ì‚¬ìš©í•  ë°ëª¨ë¥¼ ì„ íƒí•˜ì„¸ìš”:\",\n",
    "        [\"Amazon Bedrock Agent\", \"Amazon Bedrock Knowledge Base\"]\n",
    "    )\n",
    "    \n",
    "    # ì„ íƒì— ë”°ë¼ ì¶”ê°€ ì •ë³´ í‘œì‹œ\n",
    "    if selected_demo == \"Amazon Bedrock Agent\":\n",
    "        st.header(\"Agent ì •ë³´\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Agent ID**: {AGENT_ID}\")\n",
    "        st.markdown(f\"**ì„¸ì…˜ ID**: {st.session_state.session_id}\")\n",
    "        st.markdown(f\"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        st.header(\"Knowledge Base ì •ë³´\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Knowledge Base ID**: {VECTOR_RAG_KB_ID} (Vector), {GRAPH_RAG_KB_ID} (Graph)\")\n",
    "        st.markdown(f\"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if st.button(\"ìƒˆ ëŒ€í™” ì‹œì‘\"):\n",
    "        st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "# Amazon Bedrock Agent ê¸°ëŠ¥\n",
    "def run_agent_chatbot():\n",
    "    # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Semantic ê²€ìƒ‰ ìœ í˜• íŒë³„ í•¨ìˆ˜\n",
    "    def determine_search_type_semantic(query):\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê²€ìƒ‰ ë°©ì‹ì„ ì¶”ì²œí•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒì€ ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤:\n",
    "        1. GraphRAG: ì—”í‹°í‹° ê°„ì˜ ê´€ê³„, ì—°ê²° êµ¬ì¡°, ë„¤íŠ¸ì›Œí¬ ë¶„ì„, ê²½ë¡œ íƒìƒ‰, ì§€ì‹ ê·¸ë˜í”„ íŒ¨í„´ ë“± ê´€ê³„í˜• ë°ì´í„°ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        2. VectorRAG: ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰, í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆì˜, ë¬¸ì„œ ë‚´ìš© ìš”ì•½, ìœ ì‚¬ ê°œë… íƒìƒ‰ ë“± ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , \"GraphRAG\" ë˜ëŠ” \"VectorRAG\" ì¤‘ ë” ì í•©í•œ ê²€ìƒ‰ ë°©ì‹ì„ ê²°ì •í•œ í›„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "        \n",
    "        ê²€ìƒ‰ë°©ì‹: [GraphRAG ë˜ëŠ” VectorRAG] \\n\n",
    "        ê·¼ê±°: [ì´ ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•œ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…]\n",
    "        \n",
    "        ì§ˆë¬¸: \"{query}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Amazon Bedrockì˜ Claude ëª¨ë¸ì„ ì‚¬ìš©\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=\"anthropic.claude-instant-v1\",\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 200,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            response_text = response_body[\"completion\"].strip()\n",
    "            \n",
    "            # ì‘ë‹µì—ì„œ ê²€ìƒ‰ ë°©ì‹ê³¼ ê·¼ê±° ì¶”ì¶œ\n",
    "            lines = response_text.split('\\n')\n",
    "            search_type = \"VectorRAG\"  # ê¸°ë³¸ê°’\n",
    "            reason = \"ê¸°ë³¸ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤.\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith(\"ê²€ìƒ‰ë°©ì‹:\"):\n",
    "                    search_type_text = line.replace(\"ê²€ìƒ‰ë°©ì‹:\", \"\").strip()\n",
    "                    if \"GraphRAG\" in search_type_text:\n",
    "                        search_type = \"GraphRAG\"\n",
    "                    else:\n",
    "                        search_type = \"VectorRAG\"\n",
    "                elif line.startswith(\"ê·¼ê±°:\"):\n",
    "                    reason = line.replace(\"ê·¼ê±°:\", \"\").strip()\n",
    "            \n",
    "            return search_type, reason\n",
    "        except Exception as e:\n",
    "            st.error(f\"ê²€ìƒ‰ ìœ í˜• ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            return \"VectorRAG\", \"ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê¸°ë³¸ ê²€ìƒ‰ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "    # Bedrock Agentì— ì¿¼ë¦¬ ì „ì†¡ í•¨ìˆ˜\n",
    "    def query_bedrock_agent(query, search_type, reason):\n",
    "        try:\n",
    "            response = bedrock_agent_runtime.invoke_agent(\n",
    "                agentId=AGENT_ID,\n",
    "                agentAliasId=AGENT_ALIAS_ID,\n",
    "                sessionId=st.session_state.session_id,\n",
    "                inputText=query,\n",
    "                enableTrace=False\n",
    "            )\n",
    "            \n",
    "            # ì‘ë‹µ ì²˜ë¦¬\n",
    "            final_response = \"\"\n",
    "            for event in response['completion']:\n",
    "                if 'chunk' in event:\n",
    "                    try:\n",
    "                        chunk_data = json.loads(event['chunk']['bytes'].decode('utf-8'))\n",
    "                        if 'content' in chunk_data:\n",
    "                            final_response += chunk_data['content']\n",
    "                    except json.JSONDecodeError:\n",
    "                        content = event['chunk']['bytes'].decode('utf-8')\n",
    "                        final_response += content\n",
    "            \n",
    "            # ê²€ìƒ‰ ìœ í˜•ê³¼ ê·¼ê±° í‘œì‹œ\n",
    "            final_response += \"\\n\\n---\"\n",
    "            final_response += f\"\\n\\nê²€ìƒ‰ ë°©ì‹: {search_type}\"\n",
    "            final_response += f\"\\n\\nì„ íƒ ê·¼ê±°: {reason}\"\n",
    "            final_response += \"\\n\\n---\"\n",
    "            \n",
    "            return final_response\n",
    "        except Exception as e:\n",
    "            return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "    # Streamlit UI ì„¤ì •\n",
    "    st.title(\"Amazon Bedrock Agent\")\n",
    "    st.markdown(\"ì¼ë°˜ Vector RAGì™€ Graph RAGë¥¼ í™œìš©í•œ Agentic ì±—ë´‡ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
    "    if prompt := st.chat_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\"):\n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìœ í˜• ê²°ì •\n",
    "        with st.spinner(\"ê²€ìƒ‰ ë°©ì‹ ë¶„ì„ ì¤‘...\"):\n",
    "            search_type, reason = determine_search_type_semantic(prompt)\n",
    "        \n",
    "        # ë¡œë”© í‘œì‹œ\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"ğŸ¤” ìƒê° ì¤‘...\")\n",
    "            \n",
    "            # Bedrock Agentì— ì¿¼ë¦¬ ì „ì†¡\n",
    "            response = query_bedrock_agent(prompt, search_type, reason)\n",
    "            \n",
    "            # ì‘ë‹µ í‘œì‹œ\n",
    "            message_placeholder.markdown(response)\n",
    "        \n",
    "        # ì±„íŒ… ê¸°ë¡ì— ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Amazon Bedrock Knowledge Base ê¸°ëŠ¥\n",
    "def run_knowledge_base_demo():\n",
    "    # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\", \n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸\n",
    "    predefined_prompts = [\n",
    "        \"Amazonì€ ìš´ì˜ ë¹„ìš©ì˜ ì¦ê°€ê°€ ë‹¤ë¥¸ ì¬ë¬´ ì§€í‘œë“¤ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì³¤ë‚˜ìš”?\",\n",
    "        \"Amazonì˜ ì´ ìˆœë§¤ì¶œì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í–ˆë‚˜ìš”?\",\n",
    "        \"Amazonì˜ ì˜¨ë¼ì¸ ì†Œë§¤ ì„œë¹„ìŠ¤ ë§¤ì¶œì€ ë¶„ê¸°ë³„ë¡œ ì–´ë–»ê²Œ ë³€ë™í–ˆë‚˜ìš”?\"\n",
    "    ]\n",
    "\n",
    "    # í˜ì´ì§€ í—¤ë”\n",
    "    st.title(\"Amazon Bedrock Knowledge Base\")\n",
    "    st.markdown(\"ì¼ë°˜ Vector RAGì™€ Graph RAG ì ‘ê·¼ ë°©ì‹ ê°„ì˜ ì‘ë‹µ ë¹„êµ\")\n",
    "\n",
    "    # Knowledge Baseì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "    def retrieve_from_knowledge_base(query, kb_id, number_of_results=3):\n",
    "        try:\n",
    "            # KB IDì— ë”°ë¼ ê²€ìƒ‰ ìœ í˜• ê²°ì •\n",
    "            search_type = \"SEMANTIC\" if kb_id == GRAPH_RAG_KB_ID else \"HYBRID\"\n",
    "            \n",
    "            # Knowledge Baseì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "            response = bedrock_agent_runtime.retrieve(\n",
    "                retrievalQuery={\n",
    "                    'text': query\n",
    "                },\n",
    "                knowledgeBaseId=kb_id,\n",
    "                retrievalConfiguration={\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'numberOfResults': number_of_results,\n",
    "                        'overrideSearchType': search_type\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            contexts = []\n",
    "            if 'retrievalResults' in response:\n",
    "                for result in response['retrievalResults']:\n",
    "                    if 'content' in result and 'text' in result['content']:\n",
    "                        contexts.append(result['content']['text'])\n",
    "            \n",
    "            return contexts\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # Knowledge Base ì¿¼ë¦¬ í•¨ìˆ˜\n",
    "    def query_knowledge_base(query, kb_id):\n",
    "        try:\n",
    "            # Knowledge Baseì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "            contexts = retrieve_from_knowledge_base(query, kb_id)\n",
    "            \n",
    "            if isinstance(contexts, str) and contexts.startswith(\"Error\"):\n",
    "                return contexts  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "            \n",
    "            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "            prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions by using fact based when possible.\n",
    "You're a helpful assistant who loves to respond in Korean.\n",
    "Use the following pieces of information to provide a detailed answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "            # Claude 3.7 Sonnet ì‚¬ìš©\n",
    "            model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\" \n",
    "            \n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 8192,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0\n",
    "                }),\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            # ì‘ë‹µ ì¶”ì¶œ\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            response_text = response_body.get('content')[0]['text']\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error querying Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # ì‘ë‹µì„ ìœ„í•œ ë‘ ê°œì˜ ì—´ ìƒì„±\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    with col1:\n",
    "        st.subheader(\"Vector RAG\")\n",
    "        for message in st.session_state.regular_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Graph RAG\")\n",
    "        for message in st.session_state.graph_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©ì ì„ íƒ\n",
    "    selected_prompt_index = st.selectbox(\"ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”:\", options=range(len(predefined_prompts)), format_func=lambda i: predefined_prompts[i])\n",
    "    \n",
    "    # ì„ íƒí•œ í”„ë¡¬í”„íŠ¸ ì œì¶œ ë²„íŠ¼\n",
    "    if st.button(\"ì§ˆë¬¸ ì œì¶œ\"):\n",
    "        # ì„ íƒí•œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° - ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤\n",
    "        user_query = predefined_prompts[selected_prompt_index]\n",
    "        \n",
    "        # ì´ì „ ë©”ì‹œì§€ ì§€ìš°ê¸°\n",
    "        st.session_state.regular_messages = []\n",
    "        st.session_state.graph_messages = []\n",
    "        \n",
    "        # ë‘ ì±„íŒ… ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
    "        st.session_state.regular_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state.graph_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        # ë‘ ì—´ì— ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        with col1:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        with col2:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        \n",
    "        # ë‘ ì§€ì‹ ë² ì´ìŠ¤ ì¿¼ë¦¬\n",
    "        regular_response = query_knowledge_base(user_query, VECTOR_RAG_KB_ID)\n",
    "        graph_response = query_knowledge_base(user_query, GRAPH_RAG_KB_ID)\n",
    "        \n",
    "        # ì±„íŒ… ê¸°ë¡ì— ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.regular_messages.append({\"role\": \"assistant\", \"content\": regular_response})\n",
    "        st.session_state.graph_messages.append({\"role\": \"assistant\", \"content\": graph_response})\n",
    "        \n",
    "        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ í‘œì‹œ\n",
    "        with col1:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(regular_response)\n",
    "        \n",
    "        with col2:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(graph_response)\n",
    "        \n",
    "        # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì¬ì‹¤í–‰\n",
    "        st.rerun()\n",
    "\n",
    "# ë¼ë””ì˜¤ ë²„íŠ¼ ì„ íƒì— ë”°ë¼ ì„ íƒëœ ë°ëª¨ ì‹¤í–‰\n",
    "if selected_demo == \"Amazon Bedrock Agent\":\n",
    "    run_agent_chatbot()\n",
    "else:\n",
    "    run_knowledge_base_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì±—ë´‡ ì‹¤í–‰ì„ ìœ„í•œ requirements ì„¤ì¹˜ ë° ì‹¤í–‰\n",
    "\n",
    "- ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰ í•˜ê³  í˜„ì¬ SageMaker ë…¸íŠ¸ë¶ì´ ì—´ë ¤ìˆëŠ” ì›¹ ë¸Œë¼ìš°ì €ì˜ URLì„ ë³µì‚¬í•˜ê³  URL ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì • í›„ ì‹¤í–‰ ê°€ëŠ¥\n",
    "\n",
    "```python\n",
    "https://rag-test.studio.us-west-2.sagemaker.aws/jupyterlab/default/[ì¶”ê°€]proxy/8080/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•± ì‹¤í–‰\n",
    "!streamlit run ../../app.py --server.port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Option - ì½”ë“œ ì´í•´í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³  AWS ë¦¬ì†ŒìŠ¤ IDì™€ ë¦¬ì „ ì„¤ì •ì„ ìœ„í•œ ë³€ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from botocore.client import Config\n",
    "\n",
    "# AWS ì„¤ì • ë³€ìˆ˜\n",
    "AWS_REGION = \"us-west-2\" #ì‚¬ìš©ì¤‘ì¸ Regionìœ¼ë¡œ ë³€ê²½\n",
    "AGENT_ID = \"AR14QVDQII\" #Agent_IDë¡œ ìˆ˜ì •\n",
    "AGENT_ALIAS_ID = \"TSTALIASID\" #Agent alias idëŠ” í˜„ì¬ ê°’ìœ¼ë¡œ ìœ ì§€ \"TSTALIASID\"\n",
    "VECTOR_RAG_KB_ID = \"VXVR4W9Y2O\" #VectorRAG KB IDë¡œ ìˆ˜ì •\n",
    "GRAPH_RAG_KB_ID = \"DBXNEKHXD4\" #GraphRAG KB IDë¡œ ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í˜ì´ì§€ ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n",
    "Streamlit í˜ì´ì§€ë¥¼ ì„¤ì •í•˜ê³  ëŒ€í™” ì„¸ì…˜ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜ì´ì§€ ì„¤ì •\n",
    "st.set_page_config(page_title=\"Amazon Bedrock RAG Demos\", layout=\"wide\")\n",
    "\n",
    "# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"regular_messages\" not in st.session_state:\n",
    "    st.session_state.regular_messages = []\n",
    "\n",
    "if \"graph_messages\" not in st.session_state:\n",
    "    st.session_state.graph_messages = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì‚¬ì´ë“œë°” UI êµ¬ì„±\n",
    "ì‚¬ì´ë“œë°”ì— ë°ëª¨ ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì„ íƒëœ ë°ëª¨ì— ë”°ë¼ ë‹¤ë¥¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì´ë“œë°”ì— ë¼ë””ì˜¤ ë²„íŠ¼ ìƒì„±\n",
    "with st.sidebar:\n",
    "    st.header(\"ë°ëª¨ ì„ íƒ\")\n",
    "    selected_demo = st.radio(\n",
    "        \"ì‚¬ìš©í•  ë°ëª¨ë¥¼ ì„ íƒí•˜ì„¸ìš”:\",\n",
    "        [\"Amazon Bedrock Agent\", \"Amazon Bedrock Knowledge Base\"]\n",
    "    )\n",
    "    \n",
    "    # ì„ íƒì— ë”°ë¼ ì¶”ê°€ ì •ë³´ í‘œì‹œ\n",
    "    if selected_demo == \"Amazon Bedrock Agent\":\n",
    "        st.header(\"Agent ì •ë³´\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Agent ID**: {AGENT_ID}\")\n",
    "        st.markdown(f\"**ì„¸ì…˜ ID**: {st.session_state.session_id}\")\n",
    "        st.markdown(f\"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        st.header(\"Knowledge Base ì •ë³´\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Knowledge Base ID**: {VECTOR_RAG_KB_ID} (Vector), {GRAPH_RAG_KB_ID} (Graph)\")\n",
    "        st.markdown(f\"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if st.button(\"ìƒˆ ëŒ€í™” ì‹œì‘\"):\n",
    "        st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Amazon Bedrock Agent RAG ì±—ë´‡\n",
    "Amazon Bedrock Agentë¥¼ í™œìš©í•œ ì±—ë´‡ ê¸°ëŠ¥ìœ¼ë¡œ, ì§ˆë¬¸ íŠ¹ì„±ì— ë”°ë¼ GraphRAGì™€ VectorRAG ê²€ìƒ‰ ë°©ì‹ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_chatbot():\n",
    "    # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Semantic ê²€ìƒ‰ ìœ í˜• íŒë³„ í•¨ìˆ˜\n",
    "    def determine_search_type_semantic(query):\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê²€ìƒ‰ ë°©ì‹ì„ ì¶”ì²œí•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒì€ ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤:\n",
    "        1. GraphRAG: ì—”í‹°í‹° ê°„ì˜ ê´€ê³„, ì—°ê²° êµ¬ì¡°, ë„¤íŠ¸ì›Œí¬ ë¶„ì„, ê²½ë¡œ íƒìƒ‰, ì§€ì‹ ê·¸ë˜í”„ íŒ¨í„´ ë“± ê´€ê³„í˜• ë°ì´í„°ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        2. VectorRAG: ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰, í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆì˜, ë¬¸ì„œ ë‚´ìš© ìš”ì•½, ìœ ì‚¬ ê°œë… íƒìƒ‰ ë“± ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , \"GraphRAG\" ë˜ëŠ” \"VectorRAG\" ì¤‘ ë” ì í•©í•œ ê²€ìƒ‰ ë°©ì‹ì„ ê²°ì •í•œ í›„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "        \n",
    "        ê²€ìƒ‰ë°©ì‹: [GraphRAG ë˜ëŠ” VectorRAG] \\n\n",
    "        ê·¼ê±°: [ì´ ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•œ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…]\n",
    "        \n",
    "        ì§ˆë¬¸: \"{query}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Amazon Bedrockì˜ Claude ëª¨ë¸ì„ ì‚¬ìš©\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=\"anthropic.claude-instant-v1\",\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 200,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            response_text = response_body[\"completion\"].strip()\n",
    "            \n",
    "            # ì‘ë‹µì—ì„œ ê²€ìƒ‰ ë°©ì‹ê³¼ ê·¼ê±° ì¶”ì¶œ\n",
    "            lines = response_text.split('\\n')\n",
    "            search_type = \"VectorRAG\"  # ê¸°ë³¸ê°’\n",
    "            reason = \"ê¸°ë³¸ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤.\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith(\"ê²€ìƒ‰ë°©ì‹:\"):\n",
    "                    search_type_text = line.replace(\"ê²€ìƒ‰ë°©ì‹:\", \"\").strip()\n",
    "                    if \"GraphRAG\" in search_type_text:\n",
    "                        search_type = \"GraphRAG\"\n",
    "                    else:\n",
    "                        search_type = \"VectorRAG\"\n",
    "                elif line.startswith(\"ê·¼ê±°:\"):\n",
    "                    reason = line.replace(\"ê·¼ê±°:\", \"\").strip()\n",
    "            \n",
    "            return search_type, reason\n",
    "        except Exception as e:\n",
    "            st.error(f\"ê²€ìƒ‰ ìœ í˜• ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            return \"VectorRAG\", \"ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê¸°ë³¸ ê²€ìƒ‰ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "    # Bedrock Agentì— ì¿¼ë¦¬ ì „ì†¡ í•¨ìˆ˜\n",
    "    def query_bedrock_agent(query, search_type, reason):\n",
    "        try:\n",
    "            response = bedrock_agent_runtime.invoke_agent(\n",
    "                agentId=AGENT_ID,\n",
    "                agentAliasId=AGENT_ALIAS_ID,\n",
    "                sessionId=st.session_state.session_id,\n",
    "                inputText=query,\n",
    "                enableTrace=False\n",
    "            )\n",
    "            \n",
    "            # ì‘ë‹µ ì²˜ë¦¬\n",
    "            final_response = \"\"\n",
    "            for event in response['completion']:\n",
    "                if 'chunk' in event:\n",
    "                    try:\n",
    "                        chunk_data = json.loads(event['chunk']['bytes'].decode('utf-8'))\n",
    "                        if 'content' in chunk_data:\n",
    "                            final_response += chunk_data['content']\n",
    "                    except json.JSONDecodeError:\n",
    "                        content = event['chunk']['bytes'].decode('utf-8')\n",
    "                        final_response += content\n",
    "            \n",
    "            # ê²€ìƒ‰ ìœ í˜•ê³¼ ê·¼ê±° í‘œì‹œ\n",
    "            final_response += \"\\n\\n---\"\n",
    "            final_response += f\"\\n\\nê²€ìƒ‰ ë°©ì‹: {search_type}\"\n",
    "            final_response += f\"\\n\\nì„ íƒ ê·¼ê±°: {reason}\"\n",
    "            final_response += \"\\n\\n---\"\n",
    "            \n",
    "            return final_response\n",
    "        except Exception as e:\n",
    "            return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "    # Streamlit UI ì„¤ì •\n",
    "    st.title(\"Amazon Bedrock Agent\")\n",
    "    st.markdown(\"GraphRAGì™€ VectorRAGë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì±—ë´‡ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
    "    if prompt := st.chat_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\"):\n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìœ í˜• ê²°ì •\n",
    "        with st.spinner(\"ê²€ìƒ‰ ë°©ì‹ ë¶„ì„ ì¤‘...\"):\n",
    "            search_type, reason = determine_search_type_semantic(prompt)\n",
    "        \n",
    "        # ë¡œë”© í‘œì‹œ\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"ğŸ¤” ìƒê° ì¤‘...\")\n",
    "            \n",
    "            # Bedrock Agentì— ì¿¼ë¦¬ ì „ì†¡\n",
    "            response = query_bedrock_agent(prompt, search_type, reason)\n",
    "            \n",
    "            # ì‘ë‹µ í‘œì‹œ\n",
    "            message_placeholder.markdown(response)\n",
    "        \n",
    "        # ì±„íŒ… ê¸°ë¡ì— ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Amazon Bedrock Knowledge Base ë¹„êµ ì±—ë´‡\n",
    "Vector RAGì™€ Graph RAG ì ‘ê·¼ ë°©ì‹ì˜ ì‘ë‹µì„ ë‚˜ë€íˆ ë¹„êµí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knowledge_base_demo():\n",
    "    # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\", \n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸\n",
    "    predefined_prompts = [\n",
    "        \"Amazonì€ ìš´ì˜ ë¹„ìš©ì˜ ì¦ê°€ê°€ ë‹¤ë¥¸ ì¬ë¬´ ì§€í‘œë“¤ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì³¤ë‚˜ìš”?\",\n",
    "        \"Amazonì˜ ì´ ìˆœë§¤ì¶œì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í–ˆë‚˜ìš”?\",\n",
    "        \"Amazonì˜ ì˜¨ë¼ì¸ ì†Œë§¤ ì„œë¹„ìŠ¤ ë§¤ì¶œì€ ë¶„ê¸°ë³„ë¡œ ì–´ë–»ê²Œ ë³€ë™í–ˆë‚˜ìš”?\"\n",
    "    ]\n",
    "\n",
    "    # í˜ì´ì§€ í—¤ë”\n",
    "    st.title(\"Amazon Bedrock Knowledge Base\")\n",
    "    st.markdown(\"ì¼ë°˜ Vector RAGì™€ Graph RAG ì ‘ê·¼ ë°©ì‹ ê°„ì˜ ì‘ë‹µ ë¹„êµ\")\n",
    "\n",
    "    # Knowledge Baseì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "    def retrieve_from_knowledge_base(query, kb_id, number_of_results=3):\n",
    "        try:\n",
    "            # KB IDì— ë”°ë¼ ê²€ìƒ‰ ìœ í˜• ê²°ì •\n",
    "            search_type = \"SEMANTIC\" if kb_id == GRAPH_RAG_KB_ID else \"HYBRID\"\n",
    "            \n",
    "            # Knowledge Baseì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "            response = bedrock_agent_runtime.retrieve(\n",
    "                retrievalQuery={\n",
    "                    'text': query\n",
    "                },\n",
    "                knowledgeBaseId=kb_id,\n",
    "                retrievalConfiguration={\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'numberOfResults': number_of_results,\n",
    "                        'overrideSearchType': search_type\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            contexts = []\n",
    "            if 'retrievalResults' in response:\n",
    "                for result in response['retrievalResults']:\n",
    "                    if 'content' in result and 'text' in result['content']:\n",
    "                        contexts.append(result['content']['text'])\n",
    "            \n",
    "            return contexts\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # Knowledge Base ì¿¼ë¦¬ í•¨ìˆ˜\n",
    "    def query_knowledge_base(query, kb_id):\n",
    "        try:\n",
    "            # Knowledge Baseì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "            contexts = retrieve_from_knowledge_base(query, kb_id)\n",
    "            \n",
    "            if isinstance(contexts, str) and contexts.startswith(\"Error\"):\n",
    "                return contexts  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "            \n",
    "            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "            prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions by using fact based when possible.\n",
    "You're a helpful assistant who loves to respond in Korean.\n",
    "Use the following pieces of information to provide a detailed answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "            # Claude 5 Sonnet ì‚¬ìš©\n",
    "            model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\" \n",
    "            \n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 8192,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0\n",
    "                }),\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            # ì‘ë‹µ ì¶”ì¶œ\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            response_text = response_body.get('content')[0]['text']\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error querying Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # ì‘ë‹µì„ ìœ„í•œ ë‘ ê°œì˜ ì—´ ìƒì„±\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    with col1:\n",
    "        st.subheader(\"Vector RAG\")\n",
    "        for message in st.session_state.regular_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Graph RAG\")\n",
    "        for message in st.session_state.graph_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©ì ì„ íƒ\n",
    "    selected_prompt_index = st.selectbox(\"ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”:\", options=range(len(predefined_prompts)), format_func=lambda i: predefined_prompts[i])\n",
    "    \n",
    "    # ì„ íƒí•œ í”„ë¡¬í”„íŠ¸ ì œì¶œ ë²„íŠ¼\n",
    "    if st.button(\"ì§ˆë¬¸ ì œì¶œ\"):\n",
    "        user_query = predefined_prompts[selected_prompt_index]\n",
    "        \n",
    "        # ì´ì „ ë©”ì‹œì§€ ì§€ìš°ê¸°\n",
    "        st.session_state.regular_messages = []\n",
    "        st.session_state.graph_messages = []\n",
    "        \n",
    "        # ë‘ ì±„íŒ… ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
    "        st.session_state.regular_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state.graph_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        # ë‘ ì—´ì— ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        with col1:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        with col2:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        \n",
    "        # ë‘ ì§€ì‹ ë² ì´ìŠ¤ ì¿¼ë¦¬\n",
    "        regular_response = query_knowledge_base(user_query, VECTOR_RAG_KB_ID)\n",
    "        graph_response = query_knowledge_base(user_query, GRAPH_RAG_KB_ID)\n",
    "        \n",
    "        # ì±„íŒ… ê¸°ë¡ì— ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.regular_messages.append({\"role\": \"assistant\", \"content\": regular_response})\n",
    "        st.session_state.graph_messages.append({\"role\": \"assistant\", \"content\": graph_response})\n",
    "        \n",
    "        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ í‘œì‹œ\n",
    "        with col1:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(regular_response)\n",
    "        \n",
    "        with col2:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(graph_response)\n",
    "        \n",
    "        # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì¬ì‹¤í–‰\n",
    "        st.rerun()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì±—ë´‡ ì‹¤í–‰ ë©”ì¸ ë¡œì§\n",
    "ì‚¬ìš©ìê°€ ì„ íƒí•œ ë°ëª¨ì— ë”°ë¼ í•´ë‹¹í•˜ëŠ” ê¸°ëŠ¥ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ ë¡œì§ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ë””ì˜¤ ë²„íŠ¼ ì„ íƒì— ë”°ë¼ ì„ íƒëœ ë°ëª¨ ì‹¤í–‰\n",
    "if selected_demo == \"Amazon Bedrock Agent\":\n",
    "    run_agent_chatbot()\n",
    "else:\n",
    "    run_knowledge_base_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì±—ë´‡ ì‹¤í–‰ íŒŒì¼ ìƒì„±\n",
    "Amazon Bedrock Agentì™€ Knowledge baseë¡œ êµ¬ì„±ëœ RAG ì• í”Œë¦¬ì¼€ì´ì…˜ì„ Streamlit UI ê¸°ë°˜ì˜ ì±—ë´‡ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í•´ë³¼ ìˆ˜ ìˆë„ë¡ êµ¬í˜„.\n",
    "\n",
    "- ì¤€ë¹„ ì‚¬í•­\n",
    "    - Claude Sonnet 3.5 v2 ëª¨ë¸ & Claude instant v1 ëª¨ë¸ í™œì„±í™” í•„ìš”\n",
    "    - ì•„ë˜ ì½”ë“œì—ì„œ **AGENT_ID, VECTOR_RAG_KB_ID, GRAPH_RAG_KB_ID** í•­ëª©ì„ í˜„ì¬ êµ¬ì„±í•œ ì •ë³´ë¡œ ë³€ê²½ í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../app.py\n",
    "\n",
    "import streamlit as st\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from botocore.client import Config\n",
    "\n",
    "# AWS ì„¤ì • ë³€ìˆ˜\n",
    "AWS_REGION = \"us-west-2\"\n",
    "AGENT_ALIAS_ID = \"TSTALIASID\"\n",
    "\n",
    "AGENT_ID = \"AR14QVDQII\" #agent idë¡œ ìˆ˜ì •\n",
    "VECTOR_RAG_KB_ID = \"VXVR4W9Y2O\" #Vector KB IDë¡œ ìˆ˜ì •\n",
    "GRAPH_RAG_KB_ID = \"DBXNEKHXD4\" #RAG KB IDë¡œ ìˆ˜ì •\n",
    "\n",
    "# í˜ì´ì§€ ì„¤ì •\n",
    "st.set_page_config(page_title=\"Amazon Bedrock Demos\", layout=\"wide\")\n",
    "\n",
    "# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"regular_messages\" not in st.session_state:\n",
    "    st.session_state.regular_messages = []\n",
    "\n",
    "if \"graph_messages\" not in st.session_state:\n",
    "    st.session_state.graph_messages = []\n",
    "\n",
    "# ì‚¬ì´ë“œë°”ì— ë¼ë””ì˜¤ ë²„íŠ¼ ìƒì„±\n",
    "with st.sidebar:\n",
    "    st.header(\"ë°ëª¨ ì„ íƒ\")\n",
    "    selected_demo = st.radio(\n",
    "        \"ì‚¬ìš©í•  ë°ëª¨ë¥¼ ì„ íƒí•˜ì„¸ìš”:\",\n",
    "        [\"Amazon Bedrock Agent\", \"Amazon Bedrock Knowledge Base\"]\n",
    "    )\n",
    "    \n",
    "    # ì„ íƒì— ë”°ë¼ ì¶”ê°€ ì •ë³´ í‘œì‹œ\n",
    "    if selected_demo == \"Amazon Bedrock Agent\":\n",
    "        st.header(\"Agent ì •ë³´\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Agent ID**: {AGENT_ID}\")\n",
    "        st.markdown(f\"**ì„¸ì…˜ ID**: {st.session_state.session_id}\")\n",
    "        st.markdown(f\"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        st.header(\"Knowledge Base ì •ë³´\")\n",
    "        st.markdown(f\"**AWS Region**: {AWS_REGION}\")\n",
    "        st.markdown(f\"**Knowledge Base ID**: {VECTOR_RAG_KB_ID} (Vector), {GRAPH_RAG_KB_ID} (Graph)\")\n",
    "        st.markdown(f\"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if st.button(\"ìƒˆ ëŒ€í™” ì‹œì‘\"):\n",
    "        st.session_state.session_id = f\"session_{int(time.time())}\"\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "# Amazon Bedrock Agent ê¸°ëŠ¥\n",
    "def run_agent_chatbot():\n",
    "    # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Semantic ê²€ìƒ‰ ìœ í˜• íŒë³„ í•¨ìˆ˜\n",
    "    def determine_search_type_semantic(query):\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê²€ìƒ‰ ë°©ì‹ì„ ì¶”ì²œí•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒì€ ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤:\n",
    "        1. GraphRAG: ì—”í‹°í‹° ê°„ì˜ ê´€ê³„, ì—°ê²° êµ¬ì¡°, ë„¤íŠ¸ì›Œí¬ ë¶„ì„, ê²½ë¡œ íƒìƒ‰, ì§€ì‹ ê·¸ë˜í”„ íŒ¨í„´ ë“± ê´€ê³„í˜• ë°ì´í„°ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        2. VectorRAG: ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰, í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆì˜, ë¬¸ì„œ ë‚´ìš© ìš”ì•½, ìœ ì‚¬ ê°œë… íƒìƒ‰ ë“± ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , \"GraphRAG\" ë˜ëŠ” \"VectorRAG\" ì¤‘ ë” ì í•©í•œ ê²€ìƒ‰ ë°©ì‹ì„ ê²°ì •í•œ í›„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "        \n",
    "        ê²€ìƒ‰ë°©ì‹: [GraphRAG ë˜ëŠ” VectorRAG] \\n\n",
    "        ê·¼ê±°: [ì´ ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•œ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…]\n",
    "        \n",
    "        ì§ˆë¬¸: \"{query}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Amazon Bedrockì˜ Claude ëª¨ë¸ì„ ì‚¬ìš©\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=\"anthropic.claude-instant-v1\",\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 200,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            response_text = response_body[\"completion\"].strip()\n",
    "            \n",
    "            # ì‘ë‹µì—ì„œ ê²€ìƒ‰ ë°©ì‹ê³¼ ê·¼ê±° ì¶”ì¶œ\n",
    "            lines = response_text.split('\\n')\n",
    "            search_type = \"VectorRAG\"  # ê¸°ë³¸ê°’\n",
    "            reason = \"ê¸°ë³¸ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤.\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith(\"ê²€ìƒ‰ë°©ì‹:\"):\n",
    "                    search_type_text = line.replace(\"ê²€ìƒ‰ë°©ì‹:\", \"\").strip()\n",
    "                    if \"GraphRAG\" in search_type_text:\n",
    "                        search_type = \"GraphRAG\"\n",
    "                    else:\n",
    "                        search_type = \"VectorRAG\"\n",
    "                elif line.startswith(\"ê·¼ê±°:\"):\n",
    "                    reason = line.replace(\"ê·¼ê±°:\", \"\").strip()\n",
    "            \n",
    "            return search_type, reason\n",
    "        except Exception as e:\n",
    "            st.error(f\"ê²€ìƒ‰ ìœ í˜• ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            return \"VectorRAG\", \"ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê¸°ë³¸ ê²€ìƒ‰ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "    # Bedrock Agentì— ì¿¼ë¦¬ ì „ì†¡ í•¨ìˆ˜\n",
    "    def query_bedrock_agent(query, search_type, reason):\n",
    "        try:\n",
    "            response = bedrock_agent_runtime.invoke_agent(\n",
    "                agentId=AGENT_ID,\n",
    "                agentAliasId=AGENT_ALIAS_ID,\n",
    "                sessionId=st.session_state.session_id,\n",
    "                inputText=query,\n",
    "                enableTrace=False\n",
    "            )\n",
    "            \n",
    "            # ì‘ë‹µ ì²˜ë¦¬\n",
    "            final_response = \"\"\n",
    "            for event in response['completion']:\n",
    "                if 'chunk' in event:\n",
    "                    try:\n",
    "                        chunk_data = json.loads(event['chunk']['bytes'].decode('utf-8'))\n",
    "                        if 'content' in chunk_data:\n",
    "                            final_response += chunk_data['content']\n",
    "                    except json.JSONDecodeError:\n",
    "                        content = event['chunk']['bytes'].decode('utf-8')\n",
    "                        final_response += content\n",
    "            \n",
    "            # ê²€ìƒ‰ ìœ í˜•ê³¼ ê·¼ê±° í‘œì‹œ\n",
    "            final_response += \"\\n\\n---\"\n",
    "            final_response += f\"\\n\\nê²€ìƒ‰ ë°©ì‹: {search_type}\"\n",
    "            final_response += f\"\\n\\nì„ íƒ ê·¼ê±°: {reason}\"\n",
    "            final_response += \"\\n\\n---\"\n",
    "            \n",
    "            return final_response\n",
    "        except Exception as e:\n",
    "            return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "    # Streamlit UI ì„¤ì •\n",
    "    st.title(\"Amazon Bedrock Agent\")\n",
    "    st.markdown(\"ì¼ë°˜ Vector RAGì™€ Graph RAGë¥¼ í™œìš©í•œ Agentic ì±—ë´‡ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
    "    if prompt := st.chat_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\"):\n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìœ í˜• ê²°ì •\n",
    "        with st.spinner(\"ê²€ìƒ‰ ë°©ì‹ ë¶„ì„ ì¤‘...\"):\n",
    "            search_type, reason = determine_search_type_semantic(prompt)\n",
    "        \n",
    "        # ë¡œë”© í‘œì‹œ\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"ğŸ¤” ìƒê° ì¤‘...\")\n",
    "            \n",
    "            # Bedrock Agentì— ì¿¼ë¦¬ ì „ì†¡\n",
    "            response = query_bedrock_agent(prompt, search_type, reason)\n",
    "            \n",
    "            # ì‘ë‹µ í‘œì‹œ\n",
    "            message_placeholder.markdown(response)\n",
    "        \n",
    "        # ì±„íŒ… ê¸°ë¡ì— ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Amazon Bedrock Knowledge Base ê¸°ëŠ¥\n",
    "def run_knowledge_base_demo():\n",
    "    # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "    bedrock_agent_runtime = boto3.client(\n",
    "        service_name=\"bedrock-agent-runtime\", \n",
    "        region_name=AWS_REGION,\n",
    "        config=bedrock_config\n",
    "    )\n",
    "\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸\n",
    "    predefined_prompts = [\n",
    "        \"Amazonì€ ìš´ì˜ ë¹„ìš©ì˜ ì¦ê°€ê°€ ë‹¤ë¥¸ ì¬ë¬´ ì§€í‘œë“¤ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì³¤ë‚˜ìš”?\",\n",
    "        \"Amazonì˜ ì´ ìˆœë§¤ì¶œì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í–ˆë‚˜ìš”?\",\n",
    "        \"Amazonì˜ ì˜¨ë¼ì¸ ì†Œë§¤ ì„œë¹„ìŠ¤ ë§¤ì¶œì€ ë¶„ê¸°ë³„ë¡œ ì–´ë–»ê²Œ ë³€ë™í–ˆë‚˜ìš”?\"\n",
    "    ]\n",
    "\n",
    "    # í˜ì´ì§€ í—¤ë”\n",
    "    st.title(\"Amazon Bedrock Knowledge Base\")\n",
    "    st.markdown(\"ì¼ë°˜ Vector RAGì™€ Graph RAG ì ‘ê·¼ ë°©ì‹ ê°„ì˜ ì‘ë‹µ ë¹„êµ\")\n",
    "\n",
    "    # Knowledge Baseì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "    def retrieve_from_knowledge_base(query, kb_id, number_of_results=3):\n",
    "        try:\n",
    "            # KB IDì— ë”°ë¼ ê²€ìƒ‰ ìœ í˜• ê²°ì •\n",
    "            search_type = \"SEMANTIC\" if kb_id == GRAPH_RAG_KB_ID else \"HYBRID\"\n",
    "            \n",
    "            # Knowledge Baseì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "            response = bedrock_agent_runtime.retrieve(\n",
    "                retrievalQuery={\n",
    "                    'text': query\n",
    "                },\n",
    "                knowledgeBaseId=kb_id,\n",
    "                retrievalConfiguration={\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'numberOfResults': number_of_results,\n",
    "                        'overrideSearchType': search_type\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            contexts = []\n",
    "            if 'retrievalResults' in response:\n",
    "                for result in response['retrievalResults']:\n",
    "                    if 'content' in result and 'text' in result['content']:\n",
    "                        contexts.append(result['content']['text'])\n",
    "            \n",
    "            return contexts\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # Knowledge Base ì¿¼ë¦¬ í•¨ìˆ˜\n",
    "    def query_knowledge_base(query, kb_id):\n",
    "        try:\n",
    "            # Knowledge Baseì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "            contexts = retrieve_from_knowledge_base(query, kb_id)\n",
    "            \n",
    "            if isinstance(contexts, str) and contexts.startswith(\"Error\"):\n",
    "                return contexts  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "            \n",
    "            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "            prompt = f\"\"\"\n",
    "Human: You are an advisor AI system, and provides answers to questions by using fact based when possible.\n",
    "You're a helpful assistant who loves to respond in Korean.\n",
    "Use the following pieces of information to provide a detailed answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "            # Claude 3.5 Sonnet ì‚¬ìš©\n",
    "            model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\" \n",
    "            \n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 8192,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0\n",
    "                }),\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            # ì‘ë‹µ ì¶”ì¶œ\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            response_text = response_body.get('content')[0]['text']\n",
    "            \n",
    "            return response_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error querying Knowledge Base: {str(e)}\"\n",
    "\n",
    "    # ì‘ë‹µì„ ìœ„í•œ ë‘ ê°œì˜ ì—´ ìƒì„±\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    with col1:\n",
    "        st.subheader(\"Vector RAG\")\n",
    "        for message in st.session_state.regular_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Graph RAG\")\n",
    "        for message in st.session_state.graph_messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                st.chat_message(\"user\").write(message[\"content\"])\n",
    "            else:\n",
    "                st.chat_message(\"assistant\").write(message[\"content\"])\n",
    "\n",
    "    # ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©ì ì„ íƒ\n",
    "    selected_prompt_index = st.selectbox(\"ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”:\", options=range(len(predefined_prompts)), format_func=lambda i: predefined_prompts[i])\n",
    "    \n",
    "    # ì„ íƒí•œ í”„ë¡¬í”„íŠ¸ ì œì¶œ ë²„íŠ¼\n",
    "    if st.button(\"ì§ˆë¬¸ ì œì¶œ\"):\n",
    "        # ì„ íƒí•œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° - ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤\n",
    "        user_query = predefined_prompts[selected_prompt_index]\n",
    "        \n",
    "        # ì´ì „ ë©”ì‹œì§€ ì§€ìš°ê¸°\n",
    "        st.session_state.regular_messages = []\n",
    "        st.session_state.graph_messages = []\n",
    "        \n",
    "        # ë‘ ì±„íŒ… ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
    "        st.session_state.regular_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state.graph_messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        # ë‘ ì—´ì— ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        with col1:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        with col2:\n",
    "            st.chat_message(\"user\").write(user_query)\n",
    "        \n",
    "        # ë‘ ì§€ì‹ ë² ì´ìŠ¤ ì¿¼ë¦¬\n",
    "        regular_response = query_knowledge_base(user_query, VECTOR_RAG_KB_ID)\n",
    "        graph_response = query_knowledge_base(user_query, GRAPH_RAG_KB_ID)\n",
    "        \n",
    "        # ì±„íŒ… ê¸°ë¡ì— ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.regular_messages.append({\"role\": \"assistant\", \"content\": regular_response})\n",
    "        st.session_state.graph_messages.append({\"role\": \"assistant\", \"content\": graph_response})\n",
    "        \n",
    "        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ í‘œì‹œ\n",
    "        with col1:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(regular_response)\n",
    "        \n",
    "        with col2:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(graph_response)\n",
    "        \n",
    "        # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì¬ì‹¤í–‰\n",
    "        st.rerun()\n",
    "\n",
    "# ë¼ë””ì˜¤ ë²„íŠ¼ ì„ íƒì— ë”°ë¼ ì„ íƒëœ ë°ëª¨ ì‹¤í–‰\n",
    "if selected_demo == \"Amazon Bedrock Agent\":\n",
    "    run_agent_chatbot()\n",
    "else:\n",
    "    run_knowledge_base_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì±—ë´‡ ì‹¤í–‰ì„ ìœ„í•œ requirements ì„¤ì¹˜ ë° ì‹¤í–‰\n",
    "\n",
    "- ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰ í•˜ê³  í˜„ì¬ SageMaker ë…¸íŠ¸ë¶ì´ ì—´ë ¤ìˆëŠ” ì›¹ ë¸Œë¼ìš°ì €ì˜ URLì„ ë³µì‚¬í•˜ê³  URL ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì • í›„ ì‹¤í–‰ ê°€ëŠ¥\n",
    "    - ì˜ˆì‹œ - https://rag-test.studio.us-west-2.sagemaker.aws/jupyterlab/**default/proxy/8080/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# ì•± ì‹¤í–‰\n",
    "streamlit run ../app.py --server.port 8080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
